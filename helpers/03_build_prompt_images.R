# ==============================================================================
# Title:        Prompt Builder for Image Mode
# Description:  Functions to build user message for multimodal LLM from extracted image document.
#               Uses ellmer package to format images for vision-capable models.
#               Handles automatic chunking for large documents that would exceed context window limits.
#               Uses fixed token estimates for image processing costs.
# Output:       A tibble with fields: chunk_id | page_start | page_end | user_message (list object)
# ==============================================================================

# Helper Functions ------------------------------------------------------------

#' Build Multimodal Message
#'
#' Constructs one user prompt combining project context header with image content
#'
#' @param extracted_document Output from extract_document(mode = "image").
#' @param document_type Type of document (see README for examples).
#' @param audience Target audience (see README for examples).
#' @return List of ellmer content objects.
#' @keywords internal
build_multimodal_message <- function(extracted_document, document_type, audience) {

  # Start with text context (header only, system prompt loaded separately)
  header <- context_header(document_type, audience)

  # Initialize content list with header
  content <- list(header)

  # Add each page as an image
  for (i in seq_len(nrow(extracted_document))) {
    page_num <- extracted_document$page_number[i]
    image_path <- extracted_document$image_path[i]

    # Add page label (plain string)
    content[[length(content) + 1]] <- glue("\nPage {page_num}:")

    # Validate file exists and is readable
    if (!file.exists(image_path)) {
      stop(sprintf("Image file missing for page %d: %s", page_num, image_path))
    }

    file_size <- file.size(image_path)
    if (is.na(file_size) || file_size == 0) {
      stop(sprintf("Image file invalid for page %d: %s (size: %s bytes)",
                   page_num, image_path, ifelse(is.na(file_size), "NA", "0")))
    }

    message(sprintf("Encoding page %d: %s (%.1f KB)",
                    page_num, basename(image_path), file_size / 1024))

    # Encode image with error handling
    image_content <- tryCatch({
      content_image_file(path = image_path, resize = DETAIL_SETTING)
    }, error = function(e) {
      stop(sprintf("Failed to encode page %d: %s. Error: %s",
                   page_num, image_path, conditionMessage(e)))
    })

    content[[length(content) + 1]] <- image_content
  }

  return(content)
}

#' Check Need for Chunk
#' 
#' Determines whether to chunk based on total images to send and context window
#' Note: Context window depends on the model and is set in model_config.R
#' 
#' @param extracted_document Output from extract_document(mode = "image")
#' @param document_type Type of document (see README for examples).
#' @param audience Target audience (see README for examples).
#' @return yes or no
#' @keywords internal
check_for_chunk <- function(extracted_document, document_type, audience) {
  
  # Calculate tokens for each piece of the prompt
  header <- context_header(document_type, audience) # create header that will go with chunk
  header_tokens <- estimate_tokens(header) # estimate tokens needed for header

  total_images <- nrow(extracted_document) # total no. of images to be sent
  per_image_tokens <- switch(DETAIL_SETTING, "high" = 2805, "low" = 85) # uses setting from model_config.R
  ttl_img_tokens <- total_images * per_image_tokens

  # Estimate total input tokens for entire document
  estimated_ttl_input_tokens <- SYSTEM_PROMPT_TOKENS + header_tokens + ttl_img_tokens

  # Calculate if we need to chunk
  safety_limit <- floor(CONTEXT_WINDOW_IMAGES * 0.9) # Leave room for system prompt + response
  images_per_chunk <- floor(safety_limit / per_image_tokens) # Calculates safe no. of images per API call
  if (total_images <= images_per_chunk && estimated_ttl_input_tokens <= safety_limit) {
    chunk_decision <- "no"
  } else {
    chunk_decision <- "yes"
  }
  return(list(
    chunk_decision = chunk_decision,
    total_images = total_images,
    images_per_chunk = images_per_chunk,
    per_image_tokens = per_image_tokens,
    header_tokens = header_tokens,
    estimated_ttl_input_tokens = estimated_ttl_input_tokens
  ))
}


#' Chunk Document by Image Count
#'
#' Splits content into chunks for user prompts, based on image count.
#'
#' @param extracted_document Output from extract_document(mode = "images").
#' @param document_type Type of document (see README for examples).
#' @param audience Target audience (see README for examples).
#' @param chunk_info List generated by check_for_chunk() with relevant information.
#' @return Table with chunk_id, page_start, page_end, user_message.
#' @keywords internal
chunk_by_images <- function(extracted_document, document_type, audience, chunk_info) {
  
  # Initialize empty chunks
  chunks <- list()
  chunk_id <- 1

  # Calculate number of chunks needed
  num_chunks <- ceiling(chunk_info$total_images / chunk_info$images_per_chunk) # using info passed from check_for_chunk
  message(glue(
    "Splitting {chunk_info$total_images} pages into {num_chunks} chunk(s) ",
    "({chunk_info$images_per_chunk} images per chunk)\n"
  ))

  # Split into chunks
  for (i in seq_len(num_chunks)) {
    # Calculate page range for this chunk
    page_start <- ((i - 1) * chunk_info$images_per_chunk) + 1
    page_end <- min(i * chunk_info$images_per_chunk, chunk_info$total_images)

    # Extract pages for this chunk
    chunk_pages <- extracted_document |>
      filter(page_number >= page_start, page_number <= page_end)

    # Build multimodal content
    user_message <- build_multimodal_message(
      extracted_document = chunk_pages,
      document_type = document_type,
      audience = audience
    )

    # Estimate tokens for this chunk
    chunk_image_tokens <- nrow(chunk_pages) * chunk_info$per_image_tokens # count image tokens in the chunk
    total_chunk_tokens <- SYSTEM_PROMPT_TOKENS + chunk_info$header_tokens + chunk_image_tokens # total estimate of tokens for the chunk

    message(glue(
      "  Chunk {chunk_id}: pages {page_start}-{page_end} ",
      "(~{format(total_chunk_tokens, big.mark = ',')} tokens)"
    ))

    # Store chunk info
    chunks[[length(chunks) + 1]] <- list(
      chunk_id = chunk_id,
      page_start = page_start,
      page_end = page_end,
      user_message = user_message
    )

    chunk_id <- chunk_id + 1
  }

  # Convert to tibble
  result <- tibble(
    chunk_id = sapply(chunks, `[[`, "chunk_id"),
    page_start = sapply(chunks, `[[`, "page_start"),
    page_end = sapply(chunks, `[[`, "page_end"),
    user_message = I(lapply(chunks, `[[`, "user_message"))
  )

  return(result)
}


# Main Function ---------------------------------------------------------------

#' Build Prompt for Image Mode
#'
#' Creates user prompts from extracted images. Automatically splits large documents into chunks.
#' Used only for image mode
#'
#' @param extracted_document Output from extract_document(mode = "images").
#' @param document_type Type of document (see README for examples).
#' @param audience Target audience description (see README for examples).
#' @return Table with chunk_id, page_start, page_end, and user_message (list-column with ellmer content).
#'
#' @examples
#' \dontrun{
#'   # Build prompts from slide deck
#'   slides <- extract_document(mode = "images")
#'   prompts <- build_prompt_images(
#'     extracted_document = slides,
#'     document_type = "presentation",
#'     audience = "Executive clients"
#'   )
#' }
#'
#' @export
build_prompt_images <- function(extracted_document, document_type, audience) {

  # Determine whether to chunk
  chunk_info <- check_for_chunk(extracted_document, document_type, audience)

  if(chunk_info$chunk_decision == "no" ){

    # Tell user
    message(glue(
      "Processing {chunk_info$total_images} page(s) as images ",
      "(~{format(chunk_info$estimated_ttl_input_tokens, big.mark = ',')} tokens)\n"
    ))

    # Build single chunk
    user_message <- build_multimodal_message(
      extracted_document = extracted_document,
      document_type = document_type,
      audience = audience
    )
    # Get single output
    result <- tibble(
      chunk_id = 1L,
      page_start = min(extracted_document$page_number),
      page_end = max(extracted_document$page_number),
      user_message = I(list(user_message))
    )
  } else {
    # Need to chunk the document
    result <- chunk_by_images(
      extracted_document = extracted_document,
      document_type = document_type,
      audience = audience,
      chunk_info = chunk_info
    )
  }

  return(result)
}
